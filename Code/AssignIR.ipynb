{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re , json , os , pickle , math\n",
    "from collections import Counter\n",
    "from Operator import *\n",
    "from nltk.stem import PorterStemmer , WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Dataset and Base directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = os.path.dirname(os.path.abspath(\"\"))\n",
    "\n",
    "DATASET_PATH = os.path.join(DIR_PATH, \"DataSet\")\n",
    "\n",
    "STOPWORDBBC_PATH = os.path.join(DATASET_PATH, \"stopwordbbc.txt\")\n",
    "STOPWORDTRUMP_PATH = os.path.join(DATASET_PATH, \"stopwordtrump.txt\")\n",
    "\n",
    "DATASET_BBCSPORT = os.path.join(DATASET_PATH, \"bbcsport\")\n",
    "DATASET_TRUMPSPEECHS = os.path.join(DATASET_PATH, \"trumpspeechs\")\n",
    "\n",
    "\n",
    "CODE_PATH = os.path.join(DIR_PATH, \"Code\")\n",
    "PICKLES_PATH = os.path.join(DIR_PATH, \"Pickles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Stop words and Special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile  = open(STOPWORDTRUMP_PATH, \"r\")\n",
    "\n",
    "\n",
    "stopwordtrump_list    = word_tokenize(infile.read())\n",
    "stopwordtrump_list    = [i for i in stopwordtrump_list if i]\n",
    "\n",
    "infile  = open(STOPWORDBBC_PATH, \"r\")\n",
    "\n",
    "stopwordbbc_list    = word_tokenize(infile.read())\n",
    "stopwordbbc_list    = [i for i in stopwordbbc_list if i]\n",
    "\n",
    "specialchar_list = ['.',' ',',','[',']','(',')','\"',':','?','','-']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loading Trump dataset\n",
    "\n",
    "#### 4.1 Stemming , Lemmatization & removing special characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "le = WordNetLemmatizer() \n",
    "\n",
    "for r, d, files in os.walk(DATASET_TRUMPSPEECHS):\n",
    "    stem_speeches = [None for _ in range(len(files))]\n",
    "    lema_speeches = [None for _ in range(len(files))]\n",
    "    \n",
    "    for speech in files:\n",
    "        lema_speech  = []\n",
    "        stem_speech  = []\n",
    "        speech_no    = int(re.search(r'\\d+',speech)[0])\n",
    "        infile       = open(os.path.join(DATASET_TRUMPSPEECHS,speech), \"r\")\n",
    "        content      = infile.read()\n",
    "        content      = content.split('\\n')[1]\n",
    "        content      = re.sub(r\"[^a-zA-Z0-9\\']+\", ' ', content)\n",
    "        content      = content.casefold()\n",
    "        content_list = word_tokenize(content)\n",
    "        for word in content_list:\n",
    "            stem_speech.append(ps.stem(word))\n",
    "            lema_speech.append(le.lemmatize(word))\n",
    "        stem_speeches[speech_no] = stem_speech\n",
    "        lema_speeches[speech_no] = lema_speech\n",
    "        infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Generting Posting lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_posting_list = {}\n",
    "for doc_no in range(0,len(stem_speeches)):\n",
    "    speech       = stem_speeches[doc_no]\n",
    "    clean_speech =  list(set(speech) - set(stopwordtrump_list))\n",
    "    posting_list = {}\n",
    "    for word in clean_speech:\n",
    "        if word.lower() not in stopwordtrump_list:\n",
    "            word_index = [index for index, value in enumerate(speech) if value == word]\n",
    "            posting_list[word] = [{doc_no:word_index}]\n",
    "    for word in posting_list.keys():\n",
    "        all_posting_list.setdefault(word, []).append(posting_list[word][0])\n",
    "out_file = open(os.path.join(PICKLES_PATH, \"postinglist.pickle\"), \"wb\")\n",
    "pickle.dump(all_posting_list, out_file)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Generting Vector Space Model (VSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "VSM     = []\n",
    "doc_frq = {}\n",
    "for doc_no in range(0,len(lema_speeches)):\n",
    "    speech       = lema_speeches[doc_no]\n",
    "    clean_speech =  list(set(speech) - set(stopwordtrump_list))\n",
    "    \n",
    "    doc_frq = Counter(doc_frq) + Counter(dict.fromkeys(clean_speech,1))\n",
    "    \n",
    "    term_count = {}\n",
    "    for word in clean_speech:\n",
    "        term_count.setdefault(word,0) \n",
    "        term_count[word] = term_count[word] +  speech.count(word)\n",
    "    VSM.append(term_count)\n",
    "for doc_no in range(0,len(VSM)):\n",
    "    for word in VSM[doc_no]:\n",
    "        VSM[doc_no][word] = VSM[doc_no][word]*math.log10( len(VSM) / doc_frq[word])\n",
    "out_file = open(os.path.join(PICKLES_PATH, \"vsm.pickle\"), \"wb\")\n",
    "pickle.dump(VSM, out_file)\n",
    "out_file.close()\n",
    "out_file = open(os.path.join(PICKLES_PATH, \"docfrq.pickle\"), \"wb\")\n",
    "pickle.dump(doc_frq, out_file)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for VSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query : muslim\n",
      "[3, 4, 9, 2, 20, 7, 6]\n",
      "LENGTH :  7\n"
     ]
    }
   ],
   "source": [
    "query = input('Enter Query : ')\n",
    "\n",
    "in_file = open(os.path.join(PICKLES_PATH, \"vsm.pickle\"), \"rb\")\n",
    "VSM = pickle.load(in_file)\n",
    "in_file.close()\n",
    "\n",
    "in_file = open(os.path.join(PICKLES_PATH, \"docfrq.pickle\"), \"rb\")\n",
    "doc_frq = pickle.load(in_file)\n",
    "in_file.close()\n",
    "\n",
    "le = WordNetLemmatizer() \n",
    "\n",
    "query       =  re.sub(r'[^a-zA-Z0-9_\\s]+', '', query)\n",
    "query       = query.casefold()\n",
    "query_list  = word_tokenize(query)     \n",
    "lema_query  = [le.lemmatize(word) for word in query_list]\n",
    "\n",
    "clean_query = list(set(lema_query) - set(stopwordtrump_list))\n",
    "\n",
    "query_count = {}\n",
    "for word in clean_query:\n",
    "    query_count.setdefault(word,0)\n",
    "    try:\n",
    "        query_count[word] = (query_count[word] +  lema_query.count(word))*math.log10(56 / doc_frq[word])\n",
    "    except ( KeyError , ZeroDivisionError) :\n",
    "        query_count[word] = 0.0\n",
    "ans = []\n",
    "for doc_no in range(0,len(VSM)):\n",
    "    _sum = 0.0\n",
    "    for word in clean_query:\n",
    "        try:\n",
    "            _sum = _sum + (VSM[doc_no][word] * query_count[word]) \n",
    "        except KeyError:\n",
    "            _sum = _sum + (0.0 * query_count[word])\n",
    "\n",
    "    x = [ v for k, v in VSM[doc_no].items()] \n",
    "    y = [ v for k, v in query_count.items()] \n",
    "\n",
    "    mag_x = math.sqrt(sum(x_i*x_i for x_i in x))\n",
    "    mag_y = math.sqrt(sum(y_i*y_i for y_i in y))\n",
    "    try:\n",
    "        sim   =  _sum/(mag_x*mag_y)\n",
    "    except ZeroDivisionError:\n",
    "        print(\"I'm sorry\")\n",
    "    if sim >=0.0005:\n",
    "        ans.append((sim,doc_no))\n",
    "        # print('doc_no : ',doc_no,end=' -> ')\n",
    "        # print('SIM : ',sim)\n",
    "ans  = sorted(ans,reverse=True)\n",
    "output = [item[1] for item in ans] \n",
    "print(output)\n",
    "print('LENGTH : ',len(ans))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Positional Index & Phrasal Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query : Hillary Clinton /0\n",
      "[(1, 2046, 2047), (2, 1404, 1405), (2, 1455, 1456), (3, 14, 15), (3, 1084, 1085), (3, 1116, 1117), (3, 1184, 1185), (3, 1214, 1215), (3, 1350, 1351), (3, 1397, 1398), (3, 1896, 1897), (3, 1927, 1928), (3, 1956, 1957), (3, 2070, 2071), (3, 2125, 2126), (3, 2420, 2421), (3, 2755, 2756), (3, 2850, 2851), (3, 3039, 3040), (4, 148, 149), (4, 354, 355), (4, 571, 572), (4, 704, 705), (4, 1028, 1029), (4, 1030, 1029), (4, 1030, 1031), (4, 1076, 1077), (4, 1119, 1120), (4, 1137, 1138), (4, 1187, 1188), (4, 1245, 1246), (4, 1441, 1442), (4, 1476, 1477), (4, 1504, 1505), (4, 1540, 1541), (4, 1623, 1624), (4, 1707, 1708), (4, 1727, 1728), (4, 1828, 1829), (4, 1869, 1870), (4, 1929, 1930), (4, 2025, 2026), (4, 2044, 2045), (4, 2159, 2160), (4, 2198, 2199), (4, 2275, 2276), (4, 2305, 2306), (4, 2328, 2329), (4, 2399, 2400), (4, 2441, 2442), (4, 2558, 2559), (4, 2669, 2670), (4, 2695, 2696), (4, 2810, 2811), (4, 2895, 2896), (4, 3086, 3087), (4, 3269, 3270), (5, 490, 491), (5, 766, 767), (5, 1998, 1999), (5, 2018, 2019), (5, 2029, 2030), (5, 2056, 2057), (5, 2094, 2095), (5, 2277, 2278), (5, 2331, 2332), (5, 2357, 2358), (5, 2399, 2400), (6, 630, 631), (6, 761, 762), (6, 946, 947), (6, 1065, 1066), (6, 3513, 3514), (6, 3612, 3613), (7, 903, 904), (7, 999, 1000), (7, 1111, 1112), (7, 1119, 1120), (7, 1410, 1411), (7, 2525, 2526), (7, 3299, 3300), (7, 3311, 3312), (7, 3497, 3498), (7, 3852, 3853), (8, 10, 11), (8, 135, 136), (8, 1220, 1221), (8, 1399, 1400), (8, 1848, 1849), (8, 1868, 1869), (8, 1915, 1916), (8, 2049, 2050), (8, 2871, 2872), (8, 3043, 3044), (8, 4727, 4728), (8, 4770, 4771), (8, 5625, 5626), (8, 5913, 5914), (8, 5973, 5974), (8, 6004, 6005), (8, 6779, 6780), (8, 9015, 9016), (8, 9171, 9172), (8, 9617, 9618), (8, 9635, 9636), (8, 9695, 9696), (8, 9759, 9760), (8, 9844, 9845), (8, 9870, 9871), (8, 9996, 9997), (8, 10059, 10060), (8, 10288, 10289), (8, 10526, 10527), (8, 10535, 10536), (8, 10646, 10647), (9, 1360, 1361), (9, 1409, 1410), (9, 1435, 1436), (9, 1500, 1501), (9, 1936, 1937), (9, 2030, 2031), (9, 2822, 2823), (9, 3923, 3924), (9, 4202, 4203), (10, 235, 236), (10, 280, 281), (10, 594, 595), (10, 754, 755), (10, 783, 784), (10, 799, 800), (10, 1259, 1260), (10, 1316, 1317), (10, 1424, 1425), (10, 1664, 1665), (10, 2004, 2005), (10, 2054, 2055), (10, 2274, 2275), (10, 2449, 2450), (10, 2720, 2721), (11, 974, 975), (11, 1116, 1117), (11, 1144, 1145), (11, 1170, 1171), (11, 1187, 1188), (11, 1254, 1255), (11, 1352, 1353), (11, 1396, 1397), (11, 1426, 1427), (11, 1454, 1455), (11, 1474, 1475), (11, 1564, 1565), (11, 1673, 1674), (11, 1707, 1708), (11, 1951, 1952), (11, 2123, 2124), (11, 2401, 2402), (11, 2516, 2517), (11, 2905, 2906), (11, 3025, 3026), (11, 3189, 3190), (12, 544, 545), (12, 794, 795), (12, 974, 975), (12, 1029, 1030), (12, 1073, 1074), (12, 1180, 1181), (12, 1183, 1184), (12, 1265, 1266), (12, 1329, 1330), (12, 1350, 1351), (12, 1366, 1367), (12, 1539, 1540), (12, 1547, 1548), (12, 1662, 1663), (12, 1871, 1872), (12, 1970, 1971), (12, 2129, 2130), (12, 2253, 2254), (14, 95, 96), (14, 101, 102), (14, 139, 140), (14, 219, 220), (14, 285, 286), (14, 411, 412), (14, 639, 640), (14, 784, 785), (14, 885, 886), (14, 927, 928), (14, 1207, 1208), (14, 1247, 1248), (16, 421, 422), (16, 507, 508), (16, 693, 694), (16, 753, 754), (16, 848, 849), (16, 964, 965), (16, 1003, 1004), (16, 1092, 1093), (16, 1192, 1193), (16, 1314, 1315), (16, 1340, 1341), (16, 1393, 1394), (16, 1405, 1406), (16, 1535, 1536), (16, 1634, 1635), (16, 2089, 2090), (17, 391, 392), (17, 510, 511), (17, 527, 528), (17, 548, 549), (17, 685, 686), (17, 716, 717), (17, 958, 959), (17, 972, 973), (17, 1021, 1022), (17, 2002, 2003), (17, 2043, 2044), (18, 57, 58), (18, 107, 108), (18, 127, 128), (18, 254, 255), (18, 346, 347), (18, 392, 393), (18, 466, 467), (18, 850, 851), (18, 873, 874), (18, 887, 888), (19, 431, 432), (19, 1873, 1874), (19, 2024, 2025), (19, 2465, 2466), (19, 2954, 2955), (19, 3327, 3328), (19, 3503, 3504), (19, 4120, 4121), (19, 4174, 4175), (19, 4329, 4330), (19, 4437, 4438), (19, 4678, 4679), (19, 4868, 4869), (19, 5076, 5077), (19, 5305, 5306), (20, 349, 350), (20, 415, 416), (20, 441, 442), (20, 555, 556), (20, 611, 612), (20, 707, 708), (20, 867, 868), (20, 876, 877), (20, 894, 895), (20, 945, 946), (20, 1036, 1037), (20, 1122, 1123), (20, 1156, 1157), (20, 1248, 1249), (20, 1365, 1366), (20, 1464, 1465), (20, 1595, 1596), (20, 1611, 1612), (20, 1634, 1635), (20, 1668, 1669), (20, 1689, 1690), (20, 1738, 1739), (20, 1780, 1781), (20, 1972, 1973), (20, 2105, 2106), (20, 2444, 2445), (20, 2568, 2569), (21, 35, 36), (21, 180, 181), (21, 198, 199), (21, 550, 551), (21, 577, 578), (21, 598, 599), (21, 611, 612), (21, 647, 648), (21, 702, 703), (21, 727, 728), (21, 903, 904), (21, 918, 919), (21, 1056, 1057), (21, 1177, 1178), (21, 1395, 1396), (22, 394, 395), (22, 406, 407), (22, 462, 463), (22, 515, 516), (22, 531, 532), (22, 570, 571), (22, 612, 613), (22, 698, 699), (22, 749, 750), (22, 849, 850), (22, 875, 876), (22, 894, 895), (22, 941, 942), (22, 972, 973), (22, 1001, 1002), (22, 1194, 1195), (22, 1218, 1219), (24, 182, 183), (24, 360, 361), (24, 415, 416), (24, 498, 499), (24, 551, 552), (24, 919, 920), (25, 457, 458), (25, 557, 558), (25, 583, 584), (25, 660, 661), (25, 788, 789), (25, 899, 900), (25, 958, 959), (25, 1040, 1041), (25, 1171, 1172), (26, 518, 519), (26, 530, 531), (26, 731, 732), (26, 1937, 1938), (26, 2016, 2017), (26, 2258, 2259), (27, 750, 751), (27, 772, 773), (28, 625, 626), (28, 645, 646), (28, 718, 719), (28, 828, 829), (28, 857, 858), (28, 1228, 1229), (29, 215, 216), (29, 447, 448), (29, 499, 500), (29, 530, 531), (29, 580, 581), (29, 701, 702), (29, 824, 825), (29, 868, 869), (29, 889, 890), (29, 930, 931), (29, 962, 963), (29, 996, 997), (29, 1081, 1082), (29, 1371, 1372), (29, 1390, 1391), (29, 1449, 1450), (29, 1507, 1508), (30, 221, 222), (30, 468, 469), (30, 514, 515), (30, 1229, 1230), (30, 1315, 1316), (30, 1340, 1341), (30, 1782, 1783), (30, 1845, 1846), (31, 1193, 1194), (31, 1391, 1392), (31, 1586, 1587), (31, 1779, 1780), (32, 405, 406), (32, 899, 900), (32, 1209, 1210), (32, 1294, 1295), (32, 1319, 1320), (32, 2120, 2121), (33, 151, 152), (33, 173, 174), (33, 190, 191), (33, 419, 420), (33, 438, 439), (33, 454, 455), (33, 466, 467), (33, 471, 472), (33, 616, 617), (33, 652, 653), (33, 746, 747), (33, 762, 763), (33, 923, 924), (33, 1032, 1033), (33, 1102, 1103), (33, 1104, 1103), (33, 1104, 1105), (33, 1118, 1119), (33, 1360, 1361), (33, 1430, 1431), (34, 45, 46), (34, 64, 65), (34, 113, 114), (34, 246, 247), (34, 300, 301), (34, 325, 326), (34, 607, 608), (34, 623, 624), (34, 703, 704), (34, 824, 825), (34, 851, 852), (34, 873, 874), (34, 892, 893), (34, 910, 911), (34, 946, 947), (34, 1185, 1186), (34, 1267, 1268), (34, 1392, 1393), (34, 1548, 1549), (35, 157, 158), (35, 183, 184), (35, 213, 214), (35, 235, 236), (35, 254, 255), (35, 269, 270), (35, 281, 282), (35, 562, 563), (35, 656, 657), (35, 835, 836), (35, 870, 871), (35, 906, 907), (35, 1104, 1105), (35, 1291, 1292), (35, 1378, 1379), (35, 1427, 1428), (36, 145, 146), (36, 196, 197), (36, 238, 239), (36, 283, 284), (36, 297, 298), (36, 340, 341), (36, 387, 388), (36, 469, 470), (36, 489, 490), (36, 562, 563), (36, 772, 773), (36, 805, 806), (36, 827, 828), (36, 846, 847), (36, 861, 862), (36, 871, 872), (36, 1041, 1042), (36, 1092, 1093), (36, 1116, 1117), (36, 1216, 1217), (36, 1392, 1393), (36, 1518, 1519), (36, 1585, 1586), (36, 1743, 1744), (36, 1807, 1808), (36, 1813, 1814), (36, 1857, 1858), (36, 1881, 1882), (36, 1931, 1932), (36, 2016, 2017), (36, 2080, 2081), (36, 2170, 2171), (37, 111, 112), (37, 860, 861), (37, 887, 888), (37, 910, 911), (37, 970, 971), (37, 995, 996), (37, 1461, 1462), (37, 1858, 1859), (37, 2133, 2134), (37, 2214, 2215), (37, 2250, 2251), (37, 2309, 2310), (37, 2372, 2373), (37, 2429, 2430), (37, 2504, 2505), (37, 2873, 2874), (38, 148, 149), (39, 186, 187), (39, 420, 421), (39, 573, 574), (39, 822, 823), (39, 1249, 1250), (39, 1251, 1250), (39, 2049, 2050), (39, 2207, 2208), (39, 2257, 2258), (39, 2353, 2354), (40, 141, 142), (40, 207, 208), (40, 368, 369), (40, 455, 456), (40, 665, 666), (40, 967, 968), (40, 969, 968), (40, 1206, 1207), (40, 1405, 1406), (40, 1935, 1936), (40, 2116, 2117), (40, 2206, 2207), (41, 87, 88), (41, 248, 249), (41, 371, 372), (41, 745, 746), (41, 747, 746), (41, 1009, 1010), (41, 1088, 1089), (41, 1107, 1108), (41, 1458, 1459), (41, 1568, 1569), (41, 1614, 1615), (41, 1689, 1690), (42, 1, 2), (42, 48, 49), (42, 178, 179), (42, 456, 457), (42, 623, 624), (42, 658, 659), (43, 508, 509), (43, 686, 687), (44, 401, 402), (44, 1936, 1937), (44, 1981, 1982), (44, 2493, 2494), (45, 132, 133), (45, 173, 174), (45, 265, 266), (45, 301, 302), (45, 326, 327), (45, 500, 501), (45, 547, 548), (45, 955, 956), (45, 1585, 1586), (46, 220, 221), (46, 257, 258), (46, 282, 283), (46, 445, 446), (46, 703, 704), (46, 970, 971), (46, 1200, 1201), (46, 1561, 1562), (47, 138, 139), (47, 575, 576), (47, 848, 849), (47, 854, 855), (47, 1414, 1415), (47, 1584, 1585), (48, 289, 290), (48, 363, 364), (48, 424, 425), (48, 976, 977), (48, 1537, 1538), (48, 1583, 1584), (49, 368, 369), (49, 516, 517), (49, 754, 755), (49, 1111, 1112), (49, 1216, 1217), (50, 609, 610), (50, 1079, 1080), (51, 163, 164), (51, 286, 287), (51, 502, 503), (51, 544, 545), (51, 1142, 1143), (51, 1266, 1267), (52, 1222, 1223), (52, 1902, 1903), (52, 1933, 1934), (53, 34, 35), (53, 357, 358), (54, 232, 233), (54, 310, 311), (54, 557, 558), (54, 570, 571), (54, 1339, 1340)]\n"
     ]
    }
   ],
   "source": [
    "query = input('Enter Query : ')\n",
    "\n",
    "in_file = open(os.path.join(PICKLES_PATH, \"postinglist.pickle\"), \"rb\")\n",
    "posting_list= pickle.load(in_file)\n",
    "in_file.close()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "try:    \n",
    "    \n",
    "    query = query.split('/')\n",
    "    k     = int(query[1]) + 1\n",
    "    query = query[0]\n",
    "    query       =  re.sub(r'[^a-zA-Z0-9_\\s]+', '', query)\n",
    "    query       = query.casefold()\n",
    "    query_list  = word_tokenize(query)     \n",
    "    clean_query = [w for w in query_list if w.lower() not in stopwordtrump_list]\n",
    "    stem_query  = [ps.stem(word) for word in clean_query]\n",
    "    \n",
    "    postinglist_1 = posting_list[stem_query[0]]\n",
    "    postinglist_2 = posting_list[stem_query[1]]\n",
    "    \n",
    "    answer = []\n",
    "    \n",
    "    while len(postinglist_1) != 0 and len(postinglist_2) != 0:\n",
    "        doc_1 = list(postinglist_1[0].keys())[0]\n",
    "        doc_2 = list(postinglist_2[0].keys())[0]\n",
    "        if doc_1 == doc_2:\n",
    "            l = []\n",
    "\n",
    "            postionlist_1 = postinglist_1[0][doc_1]\n",
    "            postionlist_2 = postinglist_2[0][doc_2]\n",
    "\n",
    "            while len(postionlist_1) != 0:\n",
    "                while len(postionlist_2) != 0:\n",
    "                    if abs(postionlist_1[0] - postionlist_2[0]) == k :\n",
    "                        l.append(postionlist_2[0])\n",
    "                    elif postionlist_2[0] > postionlist_1[0]:\n",
    "                        break\n",
    "                    postionlist_2.pop(0)\n",
    "                while len(l) != 0 and abs(l[0] - postionlist_1[0]) > k:\n",
    "                    l.pop(0)\n",
    "                for postion in l:\n",
    "                    answer.append((doc_1,postionlist_1[0],postion))\n",
    "\n",
    "                postionlist_1.pop(0)\n",
    "\n",
    "            postinglist_1.pop(0)\n",
    "            postinglist_2.pop(0)\n",
    "\n",
    "        elif int(doc_1) < int(doc_2):\n",
    "            postinglist_1.pop(0)\n",
    "\n",
    "        else:\n",
    "            postinglist_2.pop(0)\n",
    "    print(answer)\n",
    "    \n",
    "except (KeyError, ValueError,IndexError):\n",
    "    print(\"Wrong format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Inverted Index & Boolean Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query : not hammer\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 22, 23, 26, 28, 29, 30, 31, 32, 37, 38, 41, 44, 47, 48, 52, 55]\n"
     ]
    }
   ],
   "source": [
    "query = input('Enter Query : ')\n",
    "\n",
    "in_file = open(os.path.join(PICKLES_PATH, \"postinglist.pickle\"), \"rb\")\n",
    "posting_list= pickle.load(in_file)\n",
    "in_file.close()\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "query       =  re.sub(r'[^a-zA-Z0-9_\\s]+', '', query)\n",
    "query       = word_tokenize(query)     \n",
    "\n",
    "operator = ['NOT','OR','AND','(',')']\n",
    "\n",
    "postfix_query = GetPostfix(query)\n",
    "stack = []\n",
    "try:\n",
    "    for i in postfix_query:\n",
    "        if i.upper() not in operator:\n",
    "            stack.append(i)\n",
    "        elif i.upper() == 'NOT':\n",
    "            query_1 = stack.pop()\n",
    "            if type(query_1) is str:\n",
    "                query_1  = ps.stem(query_1)\n",
    "                query_1  = GetPostingList(posting_list[query_1])\n",
    "            stack.append(NOT(query_1))\n",
    "\n",
    "        elif i.upper() == 'AND':\n",
    "            query_1 = stack.pop()\n",
    "            query_2 = stack.pop()\n",
    "            if type(query_1) is str:\n",
    "                query_1  = ps.stem(query_1)\n",
    "                query_1  = GetPostingList(posting_list[query_1])\n",
    "            if type(query_2) is str:\n",
    "                query_2  = ps.stem(query_2)\n",
    "                query_2  = GetPostingList(posting_list[query_2])\n",
    "            stack.append(AND(query_1,query_2))\n",
    "\n",
    "        elif i.upper() == 'OR':\n",
    "            query_1 = stack.pop()\n",
    "            query_2 = stack.pop()\n",
    "            if type(query_1) is str:\n",
    "                query_1  = ps.stem(query_1)\n",
    "                query_1  = GetPostingList(posting_list[query_1])\n",
    "            if type(query_2) is str:\n",
    "                query_2  = ps.stem(query_2)\n",
    "                query_2  = GetPostingList(posting_list[query_2])\n",
    "            stack.append(OR(query_1,query_2))\n",
    "    answer = stack.pop()\n",
    "    if type(answer) is str:\n",
    "        answer  = ps.stem(answer)\n",
    "        answer  = GetPostingList(posting_list[answer])\n",
    "    answer.sort()\n",
    "    print(answer)\n",
    "except (KeyError ,ValueError):\n",
    "    print('wrong key words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRA CODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_file = open(os.path.join(DATASET_PATH, \"querylist.txt\"), \"r\")\n",
    "# query_list = []\n",
    "# _query = in_file.readlines()\n",
    "# _query = [q.replace('\\n','') for q in _query if q.replace('\\n','') != '']\n",
    "# i = 0\n",
    "# while 1:\n",
    "#     if i == len(_query):\n",
    "#         break\n",
    "#     query = _query[i]\n",
    "#     i = i + 1\n",
    "#     ans   =  _query[i]\n",
    "#     if ans == 'None':\n",
    "#         ans = []\n",
    "#     else:\n",
    "#         ans = [int(a) for a in ans.split(\",\")]\n",
    "#         ans.sort()\n",
    "#     query_list.append((query,ans))\n",
    "#     i = i + 1\n",
    "    \n",
    "# in_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 32-bit",
   "language": "python",
   "name": "python37432bit2932b5eee13249a5b214576e23984207"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

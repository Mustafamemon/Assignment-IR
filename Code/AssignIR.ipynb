{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re ,json , os ,pickle\n",
    "from Operator import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Dataset and Base directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = os.path.dirname(os.path.abspath(\"\"))\n",
    "\n",
    "DATASET_PATH = os.path.join(DIR_PATH, \"DataSet\")\n",
    "\n",
    "STOPWORDBBC_PATH = os.path.join(DATASET_PATH, \"stopwordbbc.txt\")\n",
    "STOPWORDTRUMP_PATH = os.path.join(DATASET_PATH, \"stopwordtrump.txt\")\n",
    "\n",
    "DATASET_BBCSPORT = os.path.join(DATASET_PATH, \"bbcsport\")\n",
    "DATASET_TRUMPSPEECHS = os.path.join(DATASET_PATH, \"trumpspeechs\")\n",
    "\n",
    "\n",
    "CODE_PATH = os.path.join(DIR_PATH, \"Code\")\n",
    "PICKLES_PATH = os.path.join(DIR_PATH, \"Pickles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Stop words and Special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile  = open(STOPWORDTRUMP_PATH, \"r\")\n",
    "\n",
    "\n",
    "stopwordtrump_list    = word_tokenize(infile.read())\n",
    "stopwordtrump_list    = [i for i in stopwordtrump_list if i]\n",
    "\n",
    "infile  = open(STOPWORDBBC_PATH, \"r\")\n",
    "\n",
    "stopwordbbc_list    = word_tokenize(infile.read())\n",
    "stopwordbbc_list    = [i for i in stopwordbbc_list if i]\n",
    "\n",
    "specialchar_list = ['.',' ',',','[',']','(',')','\"',':','?','','-']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loading Trump dataset\n",
    "\n",
    "#### 4.1 Stemming and removing special characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "for r, d, files in os.walk(DATASET_TRUMPSPEECHS):\n",
    "    speeches = [None for _ in range(len(files))]\n",
    "    for speech in files:\n",
    "        speech_no    = int(re.search(r'\\d+',speech)[0])\n",
    "        infile       = open(os.path.join(DATASET_TRUMPSPEECHS,speech), \"r\")\n",
    "        content      = infile.read()\n",
    "        content      = content.split('\\n')[1]\n",
    "        content      = re.sub(r\"[^a-zA-Z0-9\\']+\", ' ', content)\n",
    "        content_list = word_tokenize(content)\n",
    "        stem_speech  = [ps.stem(word) for word in content_list]\n",
    "        speeches[speech_no] = stem_speech\n",
    "        infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Generting Posting lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_posting_list = {}\n",
    "for doc_no in range(0,len(speeches)):\n",
    "    speech       = speeches[doc_no]\n",
    "    clean_speech =  list(set(speech) - set(stopwordtrump_list))\n",
    "    posting_list = {}\n",
    "    for word in clean_speech:\n",
    "        if word.lower() not in stopwordtrump_list:\n",
    "            word_index = [index for index, value in enumerate(speech) if value == word]\n",
    "            posting_list[word] = [{doc_no:word_index}]\n",
    "    for word in posting_list.keys():\n",
    "        all_posting_list.setdefault(word, []).append(posting_list[word][0])\n",
    "out_file = open(os.path.join(PICKLES_PATH, \"postinglist.pickle\"), \"wb\")\n",
    "pickle.dump(all_posting_list, out_file)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Positional Index & Phrasal Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query : ok\n",
      "Wrong format\n"
     ]
    }
   ],
   "source": [
    "query = input('Enter Query : ')\n",
    "\n",
    "in_file = open(os.path.join(PICKLES_PATH, \"postinglist.pickle\"), \"rb\")\n",
    "posting_list= pickle.load(in_file)\n",
    "in_file.close()\n",
    "\n",
    "try:    \n",
    "    \n",
    "    query = query.split('/')\n",
    "    k     = int(query[1]) + 1\n",
    "    query = query[0]\n",
    "    query       =  re.sub(r'[^a-zA-Z0-9_\\s]+', '', query)\n",
    "    query_list  = word_tokenize(query)     \n",
    "    clean_query = [w for w in query_list if w.lower() not in stopwordtrump_list]\n",
    "    stem_query  = [ps.stem(word) for word in clean_query]\n",
    "    \n",
    "    postinglist_1 = posting_list[stem_query[0]]\n",
    "    postinglist_2 = posting_list[stem_query[1]]\n",
    "    \n",
    "    answer = []\n",
    "    \n",
    "    while len(postinglist_1) != 0 and len(postinglist_2) != 0:\n",
    "        doc_1 = list(postinglist_1[0].keys())[0]\n",
    "        doc_2 = list(postinglist_2[0].keys())[0]\n",
    "        if doc_1 == doc_2:\n",
    "            l = []\n",
    "\n",
    "            postionlist_1 = postinglist_1[0][doc_1]\n",
    "            postionlist_2 = postinglist_2[0][doc_2]\n",
    "\n",
    "            while len(postionlist_1) != 0:\n",
    "                while len(postionlist_2) != 0:\n",
    "                    if abs(postionlist_1[0] - postionlist_2[0]) == k :\n",
    "                        l.append(postionlist_2[0])\n",
    "                    elif postionlist_2[0] > postionlist_1[0]:\n",
    "                        break\n",
    "                    postionlist_2.pop(0)\n",
    "                while len(l) != 0 and abs(l[0] - postionlist_1[0]) > k:\n",
    "                    l.pop(0)\n",
    "                for postion in l:\n",
    "                    answer.append((doc_1,postionlist_1[0],postion))\n",
    "\n",
    "                postionlist_1.pop(0)\n",
    "\n",
    "            postinglist_1.pop(0)\n",
    "            postinglist_2.pop(0)\n",
    "\n",
    "        elif int(doc_1) < int(doc_2):\n",
    "            postinglist_1.pop(0)\n",
    "\n",
    "        else:\n",
    "            postinglist_2.pop(0)\n",
    "    print(answer)\n",
    "    \n",
    "except (KeyError, ValueError,IndexError):\n",
    "    print(\"Wrong format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test for Inverted Index & Boolean Qu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query : hello world\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]\n"
     ]
    }
   ],
   "source": [
    "query = input('Enter Query : ')\n",
    "\n",
    "in_file = open(os.path.join(PICKLES_PATH, \"postinglist.pickle\"), \"rb\")\n",
    "posting_list= pickle.load(in_file)\n",
    "in_file.close()\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "query       =  re.sub(r'[^a-zA-Z0-9_\\s]+', '', query)\n",
    "query       = word_tokenize(query)     \n",
    "\n",
    "operator = ['NOT','OR','AND','(',')']\n",
    "\n",
    "postfix_query = GetPostfix(query)\n",
    "stack = []\n",
    "try:\n",
    "    for i in postfix_query:\n",
    "        if i.upper() not in operator:\n",
    "            stack.append(i)\n",
    "        elif i.upper() == 'NOT':\n",
    "            query_1 = stack.pop()\n",
    "            if type(query_1) is str:\n",
    "                query_1  = ps.stem(query_1)\n",
    "                query_1  = GetPostingList(posting_list[query_1])\n",
    "            stack.append(NOT(query_1))\n",
    "\n",
    "        elif i.upper() == 'AND':\n",
    "            query_1 = stack.pop()\n",
    "            query_2 = stack.pop()\n",
    "            if type(query_1) is str:\n",
    "                query_1  = ps.stem(query_1)\n",
    "                query_1  = GetPostingList(posting_list[query_1])\n",
    "            if type(query_2) is str:\n",
    "                query_2  = ps.stem(query_2)\n",
    "                query_2  = GetPostingList(posting_list[query_2])\n",
    "            stack.append(AND(query_1,query_2))\n",
    "\n",
    "        elif i.upper() == 'OR':\n",
    "            query_1 = stack.pop()\n",
    "            query_2 = stack.pop()\n",
    "            if type(query_1) is str:\n",
    "                query_1  = ps.stem(query_1)\n",
    "                query_1  = GetPostingList(posting_list[query_1])\n",
    "            if type(query_2) is str:\n",
    "                query_2  = ps.stem(query_2)\n",
    "                query_2  = GetPostingList(posting_list[query_2])\n",
    "            stack.append(OR(query_1,query_2))\n",
    "    answer = stack.pop()\n",
    "    if type(answer) is str:\n",
    "        answer  = ps.stem(answer)\n",
    "        answer  = GetPostingList(posting_list[answer])\n",
    "    answer.sort()\n",
    "    print(answer)\n",
    "except (KeyError ,ValueError):\n",
    "    print('wrong key words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRA CODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_file = open(os.path.join(DATASET_PATH, \"querylist.txt\"), \"r\")\n",
    "# query_list = []\n",
    "# _query = in_file.readlines()\n",
    "# _query = [q.replace('\\n','') for q in _query if q.replace('\\n','') != '']\n",
    "# i = 0\n",
    "# while 1:\n",
    "#     if i == len(_query):\n",
    "#         break\n",
    "#     query = _query[i]\n",
    "#     i = i + 1\n",
    "#     ans   =  _query[i]\n",
    "#     if ans == 'None':\n",
    "#         ans = []\n",
    "#     else:\n",
    "#         ans = [int(a) for a in ans.split(\",\")]\n",
    "#         ans.sort()\n",
    "#     query_list.append((query,ans))\n",
    "#     i = i + 1\n",
    "    \n",
    "# in_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 32-bit",
   "language": "python",
   "name": "python37432bit2932b5eee13249a5b214576e23984207"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

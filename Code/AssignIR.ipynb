{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re , json , os , pickle , math , random ,statistics\n",
    "from random import randint,seed , shuffle\n",
    "from collections import Counter\n",
    "from Operator import *\n",
    "from nltk.stem import PorterStemmer , WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Dataset and Base directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = os.path.dirname(os.path.abspath(\"\"))\n",
    "\n",
    "DATASET_PATH = os.path.join(DIR_PATH, \"DataSet\")\n",
    "\n",
    "STOPWORDBBC_PATH = os.path.join(DATASET_PATH, \"stopwordbbc.txt\")\n",
    "STOPWORDTRUMP_PATH = os.path.join(DATASET_PATH, \"stopwordtrump.txt\")\n",
    "\n",
    "DATASET_BBCSPORT = os.path.join(DATASET_PATH, \"bbcsport\")\n",
    "DATASET_TRUMPSPEECHS = os.path.join(DATASET_PATH, \"trumpspeechs\")\n",
    "\n",
    "\n",
    "CODE_PATH = os.path.join(DIR_PATH, \"Code\")\n",
    "PICKLES_PATH = os.path.join(DIR_PATH, \"Pickles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Stop words and Special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile  = open(STOPWORDTRUMP_PATH, \"r\")\n",
    "\n",
    "\n",
    "stopwordtrump_list    = word_tokenize(infile.read())\n",
    "stopwordtrump_list    = [i for i in stopwordtrump_list if i]\n",
    "\n",
    "infile  = open(STOPWORDBBC_PATH, \"r\")\n",
    "\n",
    "stopwordbbc_list    = word_tokenize(infile.read())\n",
    "stopwordbbc_list    = [i for i in stopwordbbc_list if i]\n",
    "\n",
    "specialchar_list = ['.',' ',',','[',']','(',')','\"',':','?','','-']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loading Trump dataset\n",
    "\n",
    "#### 4.1 Stemming , Lemmatization & removing special characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "le = WordNetLemmatizer() \n",
    "\n",
    "for r, d, files in os.walk(DATASET_TRUMPSPEECHS):\n",
    "    stem_speeches = [None for _ in range(len(files))]\n",
    "    lema_speeches = [None for _ in range(len(files))]\n",
    "    \n",
    "    for speech in files:\n",
    "        lema_speech  = []\n",
    "        stem_speech  = []\n",
    "        speech_no    = int(re.search(r'\\d+',speech)[0])\n",
    "        infile       = open(os.path.join(DATASET_TRUMPSPEECHS,speech), \"r\")\n",
    "        content      = infile.read()\n",
    "        content      = content.split('\\n')[1]\n",
    "        content      = re.sub(r\"[^a-zA-Z0-9\\']+\", ' ', content)\n",
    "        content      = content.casefold()\n",
    "        content_list = word_tokenize(content)\n",
    "        for word in content_list:\n",
    "            stem_speech.append(ps.stem(word))\n",
    "            lema_speech.append(le.lemmatize(word))\n",
    "        stem_speeches[speech_no] = stem_speech\n",
    "        lema_speeches[speech_no] = lema_speech\n",
    "        infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Generting Posting lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_posting_list = {}\n",
    "for doc_no in range(0,len(stem_speeches)):\n",
    "    speech       = stem_speeches[doc_no]\n",
    "    clean_speech =  list(set(speech) - set(stopwordtrump_list))\n",
    "    posting_list = {}\n",
    "    for word in clean_speech:\n",
    "        if word.lower() not in stopwordtrump_list:\n",
    "            word_index = [index for index, value in enumerate(speech) if value == word]\n",
    "            posting_list[word] = [{doc_no:word_index}]\n",
    "    for word in posting_list.keys():\n",
    "        all_posting_list.setdefault(word, []).append(posting_list[word][0])\n",
    "out_file = open(os.path.join(PICKLES_PATH, \"postinglist.pickle\"), \"wb\")\n",
    "pickle.dump(all_posting_list, out_file)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Generting Vector Space Model (VSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VSM     = []\n",
    "doc_frq = {}\n",
    "for doc_no in range(0,len(lema_speeches)):\n",
    "    speech       = lema_speeches[doc_no]\n",
    "    clean_speech =  list(set(speech) - set(stopwordtrump_list))\n",
    "    \n",
    "    doc_frq = Counter(doc_frq) + Counter(dict.fromkeys(clean_speech,1))\n",
    "    \n",
    "    term_count = {}\n",
    "    for word in clean_speech:\n",
    "        term_count.setdefault(word,0) \n",
    "        term_count[word] = term_count[word] +  speech.count(word)\n",
    "    VSM.append(term_count)\n",
    "for doc_no in range(0,len(VSM)):\n",
    "    for word in VSM[doc_no]:\n",
    "        VSM[doc_no][word] = VSM[doc_no][word]*math.log10( len(VSM) / doc_frq[word])\n",
    "out_file = open(os.path.join(PICKLES_PATH, \"vsm.pickle\"), \"wb\")\n",
    "pickle.dump(VSM, out_file)\n",
    "out_file.close()\n",
    "out_file = open(os.path.join(PICKLES_PATH, \"docfrq.pickle\"), \"wb\")\n",
    "pickle.dump(doc_frq, out_file)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Loading BBC sports dataset\n",
    "\n",
    "### K-NN\n",
    "\n",
    "#### 5.1 Split Dataset\n",
    "\n",
    "#### 5.2 Stemming  , removing special characters & stop words\n",
    "\n",
    "#### 5.3 Document frq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "percent = 70 / 100\n",
    "\n",
    "doc_frq        = {}\n",
    "all_train_doc  = {}\n",
    "all_test_doc   = {}\n",
    "actual_output  = {}\n",
    "\n",
    "for nested_folder_path, subdirs, files in os.walk(DATASET_BBCSPORT):\n",
    "    if len(subdirs) is 0:\n",
    "\n",
    "        _class                = os.path.basename(nested_folder_path)\n",
    "\n",
    "        random.shuffle(files)\n",
    "\n",
    "        train_files = math.ceil(len(files)*percent)\n",
    "\n",
    "        test_files  = len(files) - train_files \n",
    "\n",
    "        actual_output[_class] = test_files\n",
    "\n",
    "\n",
    "        for i in range(0,train_files):\n",
    "            file_path    = files[i]\n",
    "            file          = open(nested_folder_path +'\\\\'+ file_path , \"r\")\n",
    "\n",
    "            content       = file.read()\n",
    "            content       = re.sub(r\"[^a-zA-Z0-9\\']+\", ' ', content)\n",
    "            content       = content.casefold()\n",
    "            content_list  = word_tokenize(content)\n",
    "            \n",
    "            clean_content = list(set(content_list) - set(stopwordbbc_list))\n",
    "            stem_content  = [ps.stem(word) for word in clean_content] \n",
    "            \n",
    "            doc_frq       = Counter(doc_frq) + Counter(dict.fromkeys(stem_content,1))\n",
    "\n",
    "            all_train_doc.setdefault(_class, []).append(stem_content)\n",
    "        \n",
    "        for i in range(train_files,train_files+test_files):\n",
    "            file_path      = files[i]\n",
    "            file           = open(nested_folder_path +'\\\\'+ file_path , \"r\")\n",
    "\n",
    "            content       = file.read()\n",
    "            content       = re.sub(r\"[^a-zA-Z0-9\\']+\", ' ', content)\n",
    "            content       = content.casefold()\n",
    "            content_list  = word_tokenize(content)\n",
    "            \n",
    "            clean_content = list(set(content_list) - set(stopwordbbc_list))\n",
    "            stem_content  = [ps.stem(word) for word in clean_content] \n",
    "            \n",
    "            all_test_doc.setdefault(_class, []).append(stem_content)\n",
    "            \n",
    "doc_frq = Counter({k: c for k, c in doc_frq.items() if c >= 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Convert into Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vsm = GetVector(all_train_doc ,doc_frq)\n",
    "test_vsm  = GetVector(all_test_doc ,doc_frq)\n",
    "\n",
    "out_file = open(os.path.join(PICKLES_PATH, \"knntest.pickle\"), \"wb\")\n",
    "pickle.dump(test_vsm, out_file)\n",
    "out_file.close()\n",
    "out_file = open(os.path.join(PICKLES_PATH, \"knntrain.pickle\"), \"wb\")\n",
    "pickle.dump(train_vsm, out_file)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t\tEach Label Accuracy\n",
      " {'athletics': 96.66666666666667, 'cricket': 91.8918918918919, 'football': 94.9367088607595, 'rugby': 90.9090909090909, 'tennis': 96.66666666666667}\n",
      "ACCURACY :  94.0909090909091  %\n"
     ]
    }
   ],
   "source": [
    "in_file = open(os.path.join(PICKLES_PATH, \"knntrain.pickle\"), \"rb\")\n",
    "train_vsm = pickle.load(in_file)\n",
    "in_file.close()\n",
    "\n",
    "in_file = open(os.path.join(PICKLES_PATH, \"knntest.pickle\"), \"rb\")\n",
    "test_vsm = pickle.load(in_file)\n",
    "in_file.close()\n",
    "\n",
    "k              = 3\n",
    "classes        = test_vsm.keys()\n",
    "predict_output = { i:0  for i in classes} \n",
    "for test_vector in test_vsm:\n",
    "    for x in test_vsm[test_vector]:\n",
    "        sim_all_doc = [(0,0,None) for k in range(0,k)]\n",
    "        for train_vector in train_vsm:\n",
    "            for i in range(0,len(train_vsm[train_vector])):\n",
    "                y   = train_vsm[train_vector][i]\n",
    "                sim = CosineSimilarity(x,y)\n",
    "                if min(sim_all_doc)[0] <= sim:\n",
    "                    sim_all_doc[sim_all_doc.index(min(sim_all_doc))] = (sim,i,train_vector)\n",
    "\n",
    "        k_classes = [c for s,i,c in sim_all_doc]\n",
    "        try:\n",
    "            prd_class = statistics.mode(k_classes)\n",
    "        except statistics.StatisticsError:\n",
    "            prd_class = k_classes[0]\n",
    "\n",
    "        if prd_class == test_vector:\n",
    "            predict_output[test_vector] = predict_output[test_vector] + 1\n",
    "            \n",
    "## Accuracy \n",
    "each_label_accuracy = {}\n",
    "for c in actual_output:\n",
    "    try:\n",
    "        each_label_accuracy[c]  = (predict_output[c] / actual_output[c]) * 100\n",
    "    except KeyError:\n",
    "        each_label_accuracy[c] = 0\n",
    "print('\\t\\t\\t\\t\\t\\tEach Label Accuracy\\n',each_label_accuracy)\n",
    "total_test_doc = sum([len(doc) for doc in all_test_doc.values()])\n",
    "accuracy       = (sum(predict_output.values())/total_test_doc)*100\n",
    "print('ACCURACY : ',accuracy,' %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Loading BBC sports dataset\n",
    "\n",
    "### K-MEAN\n",
    "\n",
    "\n",
    "#### 6.2 Stemming  , removing special characters & stop words\n",
    "\n",
    "#### 6.3 Document frq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "all_doc_frq  = []\n",
    "all_doc      = []\n",
    "\n",
    "for nested_folder_path, subdirs, files in os.walk(DATASET_BBCSPORT):\n",
    "    if len(subdirs) is 0:\n",
    "        _class                = os.path.basename(nested_folder_path)\n",
    "        for file_path in files:\n",
    "            file          = open(nested_folder_path +'\\\\'+ file_path , \"r\")\n",
    "\n",
    "            content       = file.read()\n",
    "            content       = re.sub(r\"[^a-zA-Z0-9\\']+\", ' ', content)\n",
    "            content       = content.casefold()\n",
    "            content_list  = word_tokenize(content)\n",
    "            \n",
    "            clean_content = list(set(content_list) - set(stopwordbbc_list))\n",
    "            stem_content  = [ps.stem(word) for word in clean_content] \n",
    "            \n",
    "            all_doc_frq       = Counter(all_doc_frq) + Counter(dict.fromkeys(stem_content,1))\n",
    "\n",
    "            all_doc.append([_class]+stem_content)\n",
    "        \n",
    "all_doc_frq = Counter({k: c for k, c in all_doc_frq.items() if c >= 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 Convert into Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_doc = len(all_doc)\n",
    "all_vsm       = [] \n",
    "vocab     = all_doc_frq.keys()\n",
    "for doc_vocab in all_doc:\n",
    "    doc_vector = []\n",
    "    doc_vector = [doc_vocab.count(word)*math.log10(total_doc/ all_doc_frq[word]) for word in vocab]\n",
    "    all_vsm.append([doc_vocab[0]] +  doc_vector)\n",
    "\n",
    "\n",
    "out_file = open(os.path.join(PICKLES_PATH, \"kmean.pickle\"), \"wb\")\n",
    "pickle.dump(all_vsm, out_file)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying KMEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity :  0.6906377204884667\n"
     ]
    }
   ],
   "source": [
    "seed(1)\n",
    "\n",
    "in_file = open(os.path.join(PICKLES_PATH, \"kmean.pickle\"), \"rb\")\n",
    "all_vsm = pickle.load(in_file)\n",
    "in_file.close()\n",
    "\n",
    "shuffle(all_vsm)\n",
    "features = len(all_vsm)\n",
    "all_vsm      = numpy.array(all_vsm)\n",
    "K        = 5\n",
    "    \n",
    "iteration = 0\n",
    "if K < len(all_vsm):\n",
    "    prev_centriod = []\n",
    "\n",
    "    centriod      = all_vsm[:K]\n",
    "    centriod      = numpy.delete(centriod, 0, axis=1).astype('float64').tolist()\n",
    "\n",
    "\n",
    "    while prev_centriod != centriod:\n",
    "        iteration  +=  1\n",
    "\n",
    "        cluster     = [[] for k in range(0,K)]\n",
    "        ans_cluster = [[] for k in range(0,K)]\n",
    "\n",
    "        for vector in all_vsm:\n",
    "            x = numpy.delete(vector, 0, axis=0).astype('float64').tolist()\n",
    "            distance = []\n",
    "            for y in centriod:\n",
    "                distance.append(CosineSimilarity(x,y))\n",
    "\n",
    "\n",
    "            index = distance.index(max(distance))\n",
    "\n",
    "            cluster[index].append(x)\n",
    "\n",
    "            ans_cluster[index].append(vector[0])\n",
    "\n",
    "        prev_centriod = centriod\n",
    "        centriod      = []\n",
    "\n",
    "        for i in range(0,len(cluster)):\n",
    "\n",
    "            if len(cluster[i]) == 0:\n",
    "                ind = randint(0,len(vsm)-1)\n",
    "                x = numpy.delete(all_vsm[ind], 0, axis=0).astype('float64').tolist()\n",
    "                cluster[i].append(x)\n",
    "\n",
    "            centriod.append(Centriod(cluster[i]))\n",
    "max_count = []\n",
    "for c in ans_cluster:\n",
    "    max_count.append(c.count(statistics.mode(c)))\n",
    "    \n",
    "purity = 1/features * sum(max_count)\n",
    "\n",
    "print('Purity : ',purity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for VSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query : no patience for injustice\n",
      "[11, 7, 16, 22, 15]\n",
      "LENGTH :  5\n"
     ]
    }
   ],
   "source": [
    "query = input('Enter Query : ')\n",
    "\n",
    "in_file = open(os.path.join(PICKLES_PATH, \"vsm.pickle\"), \"rb\")\n",
    "VSM = pickle.load(in_file)\n",
    "in_file.close()\n",
    "\n",
    "in_file = open(os.path.join(PICKLES_PATH, \"docfrq.pickle\"), \"rb\")\n",
    "doc_frq = pickle.load(in_file)\n",
    "in_file.close()\n",
    "\n",
    "le = WordNetLemmatizer() \n",
    "\n",
    "query       =  re.sub(r'[^a-zA-Z0-9_\\s]+', '', query)\n",
    "query       = query.casefold()\n",
    "query_list  = word_tokenize(query)     \n",
    "lema_query  = [le.lemmatize(word) for word in query_list]\n",
    "\n",
    "clean_query = list(set(lema_query) - set(stopwordtrump_list))\n",
    "\n",
    "query_count = {}\n",
    "for word in clean_query:\n",
    "    query_count.setdefault(word,0)\n",
    "    try:\n",
    "        query_count[word] = (query_count[word] +  lema_query.count(word))*math.log10(56 / doc_frq[word])\n",
    "    except ( KeyError , ZeroDivisionError) :\n",
    "        query_count[word] = 0.0\n",
    "ans = []\n",
    "for doc_no in range(0,len(VSM)):\n",
    "    _sum = 0.0\n",
    "    for word in clean_query:\n",
    "        try:\n",
    "            _sum = _sum + (VSM[doc_no][word] * query_count[word]) \n",
    "        except KeyError:\n",
    "            _sum = _sum + (0.0 * query_count[word])\n",
    "\n",
    "    x = [ v for k, v in VSM[doc_no].items()] \n",
    "    y = [ v for k, v in query_count.items()] \n",
    "\n",
    "    mag_x = math.sqrt(sum(x_i*x_i for x_i in x))\n",
    "    mag_y = math.sqrt(sum(y_i*y_i for y_i in y))\n",
    "    try:\n",
    "        sim   =  _sum/(mag_x*mag_y)\n",
    "    except ZeroDivisionError:\n",
    "        print(\"I'm sorry\")\n",
    "    if sim >=0.0005:\n",
    "        ans.append((sim,doc_no))\n",
    "        # print('doc_no : ',doc_no,end=' -> ')\n",
    "        # print('SIM : ',sim)\n",
    "ans  = sorted(ans,reverse=True)\n",
    "output = [item[1] for item in ans] \n",
    "print(output)\n",
    "print('LENGTH : ',len(ans))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Positional Index & Phrasal Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query : keep out /2\n",
      "[(20, 696, 699), (20, 712, 715), (24, 1227, 1230), (39, 2042, 2045), (40, 1928, 1931), (51, 156, 159)]\n"
     ]
    }
   ],
   "source": [
    "query = input('Enter Query : ')\n",
    "\n",
    "in_file = open(os.path.join(PICKLES_PATH, \"postinglist.pickle\"), \"rb\")\n",
    "posting_list= pickle.load(in_file)\n",
    "in_file.close()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "try:    \n",
    "    \n",
    "    query = query.split('/')\n",
    "    k     = int(query[1]) + 1\n",
    "    query = query[0]\n",
    "    query       =  re.sub(r'[^a-zA-Z0-9_\\s]+', '', query)\n",
    "    query       = query.casefold()\n",
    "    query_list  = word_tokenize(query)     \n",
    "    clean_query = [w for w in query_list if w.lower() not in stopwordtrump_list]\n",
    "    stem_query  = [ps.stem(word) for word in clean_query]\n",
    "    \n",
    "    postinglist_1 = posting_list[stem_query[0]]\n",
    "    postinglist_2 = posting_list[stem_query[1]]\n",
    "    \n",
    "    answer = []\n",
    "    \n",
    "    while len(postinglist_1) != 0 and len(postinglist_2) != 0:\n",
    "        doc_1 = list(postinglist_1[0].keys())[0]\n",
    "        doc_2 = list(postinglist_2[0].keys())[0]\n",
    "        if doc_1 == doc_2:\n",
    "            l = []\n",
    "\n",
    "            postionlist_1 = postinglist_1[0][doc_1]\n",
    "            postionlist_2 = postinglist_2[0][doc_2]\n",
    "\n",
    "            while len(postionlist_1) != 0:\n",
    "                while len(postionlist_2) != 0:\n",
    "                    if abs(postionlist_1[0] - postionlist_2[0]) == k :\n",
    "                        l.append(postionlist_2[0])\n",
    "                    elif postionlist_2[0] > postionlist_1[0]:\n",
    "                        break\n",
    "                    postionlist_2.pop(0)\n",
    "                while len(l) != 0 and abs(l[0] - postionlist_1[0]) > k:\n",
    "                    l.pop(0)\n",
    "                for postion in l:\n",
    "                    answer.append((doc_1,postionlist_1[0],postion))\n",
    "\n",
    "                postionlist_1.pop(0)\n",
    "\n",
    "            postinglist_1.pop(0)\n",
    "            postinglist_2.pop(0)\n",
    "\n",
    "        elif int(doc_1) < int(doc_2):\n",
    "            postinglist_1.pop(0)\n",
    "\n",
    "        else:\n",
    "            postinglist_2.pop(0)\n",
    "    print(answer)\n",
    "    \n",
    "except (KeyError, ValueError,IndexError):\n",
    "    print(\"Wrong format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Inverted Index & Boolean Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query : outdated AND ( personnel OR policies)\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 31, 32, 33, 34, 36, 37, 39, 40, 42, 43, 44, 45, 48, 49, 51, 52, 54]\n"
     ]
    }
   ],
   "source": [
    "query = input('Enter Query : ')\n",
    "\n",
    "in_file = open(os.path.join(PICKLES_PATH, \"postinglist.pickle\"), \"rb\")\n",
    "posting_list= pickle.load(in_file)\n",
    "in_file.close()\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "query       =  re.sub(r'[^a-zA-Z0-9_\\s]+', '', query)\n",
    "query       = word_tokenize(query)     \n",
    "\n",
    "operator = ['NOT','OR','AND','(',')']\n",
    "\n",
    "postfix_query = GetPostfix(query)\n",
    "stack = []\n",
    "try:\n",
    "    for i in postfix_query:\n",
    "        if i.upper() not in operator:\n",
    "            stack.append(i)\n",
    "        elif i.upper() == 'NOT':\n",
    "            query_1 = stack.pop()\n",
    "            if type(query_1) is str:\n",
    "                query_1  = ps.stem(query_1)\n",
    "                query_1  = GetPostingList(posting_list[query_1])\n",
    "            stack.append(NOT(query_1))\n",
    "\n",
    "        elif i.upper() == 'AND':\n",
    "            query_1 = stack.pop()\n",
    "            query_2 = stack.pop()\n",
    "            if type(query_1) is str:\n",
    "                query_1  = ps.stem(query_1)\n",
    "                query_1  = GetPostingList(posting_list[query_1])\n",
    "            if type(query_2) is str:\n",
    "                query_2  = ps.stem(query_2)\n",
    "                query_2  = GetPostingList(posting_list[query_2])\n",
    "            stack.append(AND(query_1,query_2))\n",
    "\n",
    "        elif i.upper() == 'OR':\n",
    "            query_1 = stack.pop()\n",
    "            query_2 = stack.pop()\n",
    "            if type(query_1) is str:\n",
    "                query_1  = ps.stem(query_1)\n",
    "                query_1  = GetPostingList(posting_list[query_1])\n",
    "            if type(query_2) is str:\n",
    "                query_2  = ps.stem(query_2)\n",
    "                query_2  = GetPostingList(posting_list[query_2])\n",
    "            stack.append(OR(query_1,query_2))\n",
    "    answer = stack.pop()\n",
    "    if type(answer) is str:\n",
    "        answer  = ps.stem(answer)\n",
    "        answer  = GetPostingList(posting_list[answer])\n",
    "    answer.sort()\n",
    "    print(answer)\n",
    "except (KeyError ,ValueError):\n",
    "    print('wrong key words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRA CODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_file = open(os.path.join(DATASET_PATH, \"querylist.txt\"), \"r\")\n",
    "# query_list = []\n",
    "# _query = in_file.readlines()\n",
    "# _query = [q.replace('\\n','') for q in _query if q.replace('\\n','') != '']\n",
    "# i = 0\n",
    "# while 1:\n",
    "#     if i == len(_query):\n",
    "#         break\n",
    "#     query = _query[i]\n",
    "#     i = i + 1\n",
    "#     ans   =  _query[i]\n",
    "#     if ans == 'None':\n",
    "#         ans = []\n",
    "#     else:\n",
    "#         ans = [int(a) for a in ans.split(\",\")]\n",
    "#         ans.sort()\n",
    "#     query_list.append((query,ans))\n",
    "#     i = i + 1\n",
    "    \n",
    "# in_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 32-bit",
   "language": "python",
   "name": "python37432bit2932b5eee13249a5b214576e23984207"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

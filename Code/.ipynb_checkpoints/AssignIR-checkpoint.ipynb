{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re ,json , os ,pickle\n",
    "from Operator import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Dataset and Base directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = os.path.dirname(os.path.abspath(\"\"))\n",
    "\n",
    "DATASET_PATH = os.path.join(DIR_PATH, \"DataSet\")\n",
    "\n",
    "STOPWORDBBC_PATH = os.path.join(DATASET_PATH, \"stopwordbbc.txt\")\n",
    "STOPWORDTRUMP_PATH = os.path.join(DATASET_PATH, \"stopwordtrump.txt\")\n",
    "\n",
    "DATASET_BBCSPORT = os.path.join(DATASET_PATH, \"bbcsport\")\n",
    "DATASET_TRUMPSPEECHS = os.path.join(DATASET_PATH, \"trumpspeechs\")\n",
    "\n",
    "\n",
    "CODE_PATH = os.path.join(DIR_PATH, \"Code\")\n",
    "PICKLES_PATH = os.path.join(DIR_PATH, \"Pickles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Stop words and Special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile  = open(STOPWORDTRUMP_PATH, \"r\")\n",
    "\n",
    "\n",
    "stopwordtrump_list    = word_tokenize(infile.read())\n",
    "stopwordtrump_list    = [i for i in stopwordtrump_list if i]\n",
    "\n",
    "infile  = open(STOPWORDBBC_PATH, \"r\")\n",
    "\n",
    "stopwordbbc_list    = word_tokenize(infile.read())\n",
    "stopwordbbc_list    = [i for i in stopwordbbc_list if i]\n",
    "\n",
    "specialchar_list = ['.',' ',',','[',']','(',')','\"',':','?','','-']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loading Trump dataset\n",
    "\n",
    "#### 4.1 Stemming and removing special characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "for r, d, files in os.walk(DATASET_TRUMPSPEECHS):\n",
    "    speeches = [None for _ in range(len(files))]\n",
    "    for speech in files:\n",
    "        speech_no    = int(re.search(r'\\d+',speech)[0])\n",
    "        infile       = open(os.path.join(DATASET_TRUMPSPEECHS,speech), \"r\")\n",
    "        content      = infile.read()\n",
    "        content      = content.split('\\n')[1]\n",
    "        content      = re.sub(r\"[^a-zA-Z0-9\\']+\", ' ', content)\n",
    "        content_list = word_tokenize(content)\n",
    "        stem_speech  = [ps.stem(word) for word in content_list]\n",
    "        speeches[speech_no] = stem_speech\n",
    "        infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Generting Posting lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_posting_list = {}\n",
    "for doc_no in range(0,len(speeches)):\n",
    "    speech       = speeches[doc_no]\n",
    "    clean_speech =  list(set(speech) - set(stopwordtrump_list))\n",
    "    posting_list = {}\n",
    "    for word in clean_speech:\n",
    "        if word.lower() not in stopwordtrump_list:\n",
    "            word_index = [index for index, value in enumerate(speech) if value == word]\n",
    "            posting_list[word] = [{doc_no:word_index}]\n",
    "    for word in posting_list.keys():\n",
    "        all_posting_list.setdefault(word, []).append(posting_list[word][0])\n",
    "out_file = open(os.path.join(PICKLES_PATH, \"postinglist.pickle\"), \"wb\")\n",
    "pickle.dump(all_posting_list, out_file)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Positional Index & Phrasal Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query : develop solution /1\n",
      "[(5, 32, 34), (32, 282, 284)]\n"
     ]
    }
   ],
   "source": [
    "query = input('Enter Query : ')\n",
    "\n",
    "in_file = open(os.path.join(PICKLES_PATH, \"postinglist.pickle\"), \"rb\")\n",
    "posting_list= pickle.load(in_file)\n",
    "in_file.close()\n",
    "\n",
    "try:    \n",
    "    \n",
    "    query = query.split('/')\n",
    "    k     = int(query[1]) + 1\n",
    "    query = query[0]\n",
    "    query       =  re.sub(r'[^a-zA-Z0-9_\\s]+', '', query)\n",
    "    query_list  = word_tokenize(query)     \n",
    "    clean_query = [w for w in query_list if w.lower() not in stopwordtrump_list]\n",
    "    stem_query  = [ps.stem(word) for word in clean_query]\n",
    "    \n",
    "    postinglist_1 = posting_list[stem_query[0]]\n",
    "    postinglist_2 = posting_list[stem_query[1]]\n",
    "    \n",
    "    answer = []\n",
    "    \n",
    "    while len(postinglist_1) != 0 and len(postinglist_2) != 0:\n",
    "        doc_1 = list(postinglist_1[0].keys())[0]\n",
    "        doc_2 = list(postinglist_2[0].keys())[0]\n",
    "        if doc_1 == doc_2:\n",
    "            l = []\n",
    "\n",
    "            postionlist_1 = postinglist_1[0][doc_1]\n",
    "            postionlist_2 = postinglist_2[0][doc_2]\n",
    "\n",
    "            while len(postionlist_1) != 0:\n",
    "                while len(postionlist_2) != 0:\n",
    "                    if abs(postionlist_1[0] - postionlist_2[0]) == k :\n",
    "                        l.append(postionlist_2[0])\n",
    "                    elif postionlist_2[0] > postionlist_1[0]:\n",
    "                        break\n",
    "                    postionlist_2.pop(0)\n",
    "                while len(l) != 0 and abs(l[0] - postionlist_1[0]) > k:\n",
    "                    l.pop(0)\n",
    "                for postion in l:\n",
    "                    answer.append((doc_1,postionlist_1[0],postion))\n",
    "\n",
    "                postionlist_1.pop(0)\n",
    "\n",
    "            postinglist_1.pop(0)\n",
    "            postinglist_2.pop(0)\n",
    "\n",
    "        elif int(doc_1) < int(doc_2):\n",
    "            postinglist_1.pop(0)\n",
    "\n",
    "        else:\n",
    "            postinglist_2.pop(0)\n",
    "    print(answer)\n",
    "    \n",
    "except (KeyError, ValueError,IndexError):\n",
    "    print(\"Wrong format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'outdated AND ( personnel OR policies)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-08ae3fbcb744>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mans\u001b[0m   \u001b[1;33m=\u001b[0m  \u001b[0m_query\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mquery_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-08ae3fbcb744>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mans\u001b[0m   \u001b[1;33m=\u001b[0m  \u001b[0m_query\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mquery_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'outdated AND ( personnel OR policies)'"
     ]
    }
   ],
   "source": [
    "in_file = open(os.path.join(DATASET_PATH, \"querylist.txt\"), \"r\")\n",
    "query_list = []\n",
    "_query = in_file.readlines()\n",
    "_query = [q.replace('\\n','') for q in _query if q.replace('\\n','') != '']\n",
    "i = 0\n",
    "while 1:\n",
    "    if i == len(_query):\n",
    "        break\n",
    "    query = _query[i]\n",
    "    i = i + 1\n",
    "    ans   =  _query[i]\n",
    "    ans = [int(a) for a in ans.split(\",\")]\n",
    "    ans.sort()\n",
    "    query_list.append((query,ans))\n",
    "    i = i + 1\n",
    "    \n",
    "in_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'sort'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-aa2f2cf3a0e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'sort'"
     ]
    }
   ],
   "source": [
    "# query = input('Enter Query : ')\n",
    "\n",
    "in_file = open(os.path.join(PICKLES_PATH, \"postinglist.pickle\"), \"rb\")\n",
    "posting_list= pickle.load(in_file)\n",
    "in_file.close()\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for (query,ans) in query_list:\n",
    "    query       =  re.sub(r'[^a-zA-Z0-9_\\s]+', '', query)\n",
    "    query       = word_tokenize(query)     \n",
    "\n",
    "    operator = ['NOT','OR','AND','(',')']\n",
    "\n",
    "    postfix_query = GetPostfix(query)\n",
    "    stack = []\n",
    "    try:\n",
    "        for i in postfix_query:\n",
    "            if i.upper() not in operator:\n",
    "                stack.append(i)\n",
    "            elif i.upper() == 'NOT':\n",
    "                query_1 = stack.pop()\n",
    "                if type(query_1) is str:\n",
    "                    query_1  = ps.stem(query_1)\n",
    "                    query_1  = GetPostingList(posting_list[query_1])\n",
    "                stack.append(NOT(query_1))\n",
    "\n",
    "            elif i.upper() == 'AND':\n",
    "                query_1 = stack.pop()\n",
    "                query_2 = stack.pop()\n",
    "                if type(query_1) is str:\n",
    "                    query_1  = ps.stem(query_1)\n",
    "                    query_1  = GetPostingList(posting_list[query_1])\n",
    "                if type(query_2) is str:\n",
    "                    query_2  = ps.stem(query_2)\n",
    "                    query_2  = GetPostingList(posting_list[query_2])\n",
    "                stack.append(AND(query_1,query_2))\n",
    "\n",
    "            elif i.upper() == 'OR':\n",
    "                query_1 = stack.pop()\n",
    "                query_2 = stack.pop()\n",
    "                if type(query_1) is str:\n",
    "                    query_1  = ps.stem(query_1)\n",
    "                    query_1  = GetPostingList(posting_list[query_1])\n",
    "                if type(query_2) is str:\n",
    "                    query_2  = ps.stem(query_2)\n",
    "                    query_2  = GetPostingList(posting_list[query_2])\n",
    "                stack.append(OR(query_1,query_2))\n",
    "        answer = stack.pop()\n",
    "        if type(answer) is str:\n",
    "            answer  = ps.stem(answer)\n",
    "            answer  = GetPostingList(posting_list[answer])\n",
    "\n",
    "        answer.sort()\n",
    "        ans.sort()\n",
    "        print(answer)\n",
    "        print(ans)\n",
    "#         print(list(set(answer)-set(ans)))\n",
    "#         print(list(set(ans)-set(answer)))\n",
    "\n",
    "    except (KeyError ,ValueError):\n",
    "        print('wrong key words')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 32-bit",
   "language": "python",
   "name": "python37432bit2932b5eee13249a5b214576e23984207"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
